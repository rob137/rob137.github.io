---
layout: post
title: "Is/Ought"
date: 2026-02-18 15:30:00 +0000
tags: [ai, software, teams]
excerpt: "Your priors are stale. You just don't know it yet."
---

The rule that you should say "fewer" for countable nouns and "less" for uncountable ones? Invented by a grammarian in 1770. Before that, people used them interchangeably for centuries. An observation about formal usage hardened into a prescription about correctness. We do this constantly - slide from describing how things are to insisting that's how they ought to be.

I think this is happening right now with AI in the software lifecycle. And it's going to bite a lot of teams.

Here's the problem: your beliefs about what the models can and can't do are going stale faster than you realise. Even if you use them daily. Especially if you've been using them for a while. New state-of-the-art every four to six weeks. Constant minor releases crossing thresholds nobody anticipated. Your priors were formed on earlier versions. You tried something in September; it didn't work; you learned not to bother. Except now it works, and you're not going to find out because you've already filed it under "doesn't work."

The slide is subtle. "This is what the models can do" becomes "this is what the models should be able to do" without you noticing. Descriptive hardens into prescriptive. And then you're the person confidently explaining why something won't work while a newcomer just... does it.

I was talking to [Ian](https://www.linkedin.com/in/ian-reay/) about this. We landed on Winnie the Pooh characters as a frame for team composition:

**Eeyore** - The experienced pessimist. "It'll never work. Have we even tested these fifteen edge cases?" Ultra-conservative, catalogues failure modes, has decades of scar tissue. Incredibly valuable. Keeps you from shipping garbage.

**Tigger** - The naive optimist. "Let's try everything!" Wrong most of the time. Except when the models have moved on and they're right and you're wrong. They don't know what's supposed to be impossible, so they try it anyway.

The dangerous place is the middle. Experienced, moderately optimistic. Knows the tools well. Has opinions. The problem is: the opinions are six months out of date and they don't know it. Confidently wrong is worse than naively wrong, because the confident person doesn't check.

Ian made the point that Tiggers are like mutation testing - you inject them into the system and they explore unexpected paths. Then the Eeyores validate: did you cover these fifteen things? Yes? Fine, ship it. That combination is robust. The middle gives you neither exploration nor rigour.

So if you're building a small team right now - and teams should be small, [Brooks's law](/2026/01/03/brooks-law-redux) cuts harder than ever - consider a barbell. Don't hire three people from the middle of the distribution. Get someone who'll try things that seem stupid. Get someone who'll tell you all the ways it'll break. Let them argue.

The [funhouse mirror](/2026/02/09/funhouse-mirror) point still holds: AI tools are stretching out the differences between users. Skill matters less than freshness of assumptions and willingness to be wrong. The models keep moving. The question is whether you're updating fast enough to notice.
