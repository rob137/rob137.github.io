---
layout: post
title: "Rule of 72"
date: 2026-01-09 16:00:00 +0000
tags: [ai, llm, exponential, hardware, algorithms]
excerpt: "We're bad at exponentials. Here's how bad, and why it matters."
---

A lily pad doubles in size every day. It will cover the entire pond on day 30. On day 27, how much of the pond is covered?

12.5%.

It's easy to reason backwards: day 30 is full, so day 29 is half, day 28 is a quarter, day 27 is an eighth. But reasoning forward - from "we're on day 27 with 3 days left" to "how far along are we" - is where our intuitions fail. Three days feels like we should be close. We're barely started.

The [Rule of 72](https://en.wikipedia.org/wiki/Rule_of_72) is a mental shortcut for compound growth: divide 72 by a growth rate to get the doubling time. It applies to anything that compounds - inflation, population, computing power.

## Three Converging Curves

Jensen Huang on [No Priors](https://www.youtube.com/@NoPriorsPodcast) this week, talking about AI hardware:

> "We're seeing five to 10x every single year. Well, compounded, it's incredible. Moore's law was two times every year and a half. Over the course of five years is 10x, over the course of 10 years is 100x. In the case of AI, over the course of 10 years is probably 100,000 to a million x. And that's just the hardware."

Then he kept going:

> "The next layer is the algorithm layer... the fact that if you were to tell me that the computational burden did not go up by a factor of 10, because you're getting the compounded benefits of all three things, the hardware's going up, the algorithms of training models are going up, and of course, the model architecture's going up."

The numbers back this up.

### Hardware

![GPU Generations](/assets/images/gpu-generations.png)

H100 vs A100: 2-3x training speed, 10-20x inference speed. Blackwell claims similar jumps. Each generation comes roughly every two years. Jensen's 5-10x annual is optimistic but not unreasonable when you factor in software optimizations shipping between hardware generations.

### Algorithms

According to [Epoch AI](https://epoch.ai/blog/algorithmic-progress-in-language-models), language model algorithmic efficiency is doubling every **8 months**. That's faster than Moore's Law ever was.

DeepSeek demonstrated this dramatically. Their V3 model achieves frontier performance at a fraction of the training cost - $5.6M vs estimates of $78M+ for GPT-4. The secret? Multi-head latent attention (93% KV cache compression), mixture-of-experts architecture, and native FP8 training.

### Inference Costs

![Inference Cost Decline](/assets/images/inference-cost-decline.png)

The [Stanford AI Index](https://hai.stanford.edu/ai-index/2025-ai-index-report) reports that GPT-3.5-equivalent inference costs dropped **280x** between November 2022 and October 2024. Two years. That's not a doubling. That's eight doublings.

[Andreessen Horowitz](https://a16z.com/llmflation-llm-inference-cost/) calls this "LLMflation" - inference costs declining roughly 10x per year. And post-January 2024, Epoch AI found the median rate accelerated to 200x per year.

## The Doubling Problem

Here's a thought experiment. Something doubles every minute. How big is it at various points?

| Minute | Size |
|--------|------|
| 0 | 1 |
| 15 | 32,768 |
| 30 | ~1 billion |
| 57 | 144 quadrillion |

At minute 15: about 32 thousand. At minute 57: 144 quadrillion. The difference? 4.4 trillion times larger.

Our brains want to think "57 is about 4 times bigger than 15, so maybe the result is about 4 times bigger." Instead it's trillions of times larger.

![Doubling Problem](/assets/images/doubling-problem.png)

## We Are Provably Bad At This

This isn't just "people are bad at math." It's a specific cognitive bias with a name: [exponential growth bias](https://pmc.ncbi.nlm.nih.gov/articles/PMC8386158/).

During COVID-19, researchers ran studies. Over 90% of participants drastically underestimated infection counts after 30 days of exponential spread. More striking: the bias persisted even among highly educated participants who were *explicitly warned* about exponential growth bias. Education doesn't fix it.

[Stango and Zinman](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2009.01518.x) found people with this bias have higher debt, lower savings, and lower net worth. More concerning: they're *overconfident* about their ability to estimate exponentials. They don't seek help because they think they understand.

Why? Our heuristics evolved in environments where exponentials were rare. Linear extrapolation was usually good enough. Worrying about compound growth would have been wasted cognitive effort for most of human history.

## Projecting Forward

![Progress Scenarios](/assets/images/progress-scenarios.png)

The chart shows different scenarios over 10 years:

- **Moore's Law (~41%/year)**: The historical baseline. 10x per decade.
- **Current AI progress (~100%/year)**: What we're actually seeing. 1000x per decade.
- **Accelerated scenarios**: If the rate of progress itself increases.

Jensen's "100,000 to a million x over 10 years" would require the higher scenarios - growth rates increasing, not just compounding at current rates.

Is that plausible? [Leopold Aschenbrenner's "Situational Awareness"](https://situational-awareness.ai/) argues yes. His thesis: we're on track for transformative AI by 2027, and the exponentials compound across hardware, algorithms, and model architecture simultaneously.

I'm not making predictions. But I notice that when I try to predict intuitively, my brain reaches for linear extrapolation. That's the bias. The numbers don't care what feels reasonable.

## The Second Half of the Chessboard

There's an old story about a king and the inventor of chess. The inventor asks for one grain of wheat on the first square, two on the second, four on the third, doubling each time. The king laughs at the modest request.

The first half of the chessboard contains about 4 billion grains. The second half contains over 4 billion times more than the first half. Over 18 quintillion grains total. A heap larger than Mount Everest.

Ray Kurzweil calls this "the second half of the chessboard" - the point where exponential growth becomes economically significant. Before that point, the change seems gradual. After it, the change seems sudden.

We might already be there with AI. Or we might be on square 25, with 39 squares still to go.

Either way, my intuitions are probably wrong. So are yours. That's the uncomfortable takeaway. We can do the math, but we can't feel it.

---

*This piece is deliberately crude. If you want comprehensive analysis, see Leopold Aschenbrenner's [Situational Awareness](https://situational-awareness.ai/), the [Stanford AI Index](https://hai.stanford.edu/ai-index/2025-ai-index-report), or [Epoch AI's research](https://epoch.ai/research).*
