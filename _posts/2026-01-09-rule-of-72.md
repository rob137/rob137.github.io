---
layout: post
title: "Rule of 72"
date: 2026-01-09 16:00:00 +0000
tags: [ai, llm, exponential, hardware, algorithms]
excerpt: "A colleague and I did some back-of-envelope math. It was more interesting than I expected."
---

A colleague and I were talking about local models. Enterprise reticence around sending data to external APIs is real, but open-source alone doesn't solve it - models need to actually run locally, which means they need to be efficient enough. The hardware will improve, sure. But could today's state-of-the-art ever run on a phone?

We looked up the Tensor G4 to G5 improvement (Pixel 9 to Pixel 10). About 60% better NPU performance. Bigger than we expected, but not transformative. One generation isn't going to bridge the gap between a phone and a datacenter.

Then I mentioned the [Rule of 72](https://en.wikipedia.org/wiki/Rule_of_72).

## Compounding Does Horrifying Things

The Rule of 72 is a shortcut: divide 72 by a growth rate to get the doubling time. At 10% annual growth, you double in about 7 years. At 60% annual growth (like that Pixel improvement), you double in just over a year.

Five years of 60% annual improvement isn't 5 Ã— 60% = 300%. It's 1.6^5 = 10x.

Ten years? 100x.

That's just hardware. Jensen Huang on [No Priors](https://www.youtube.com/@NoPriorsPodcast) this week pointed out there are three curves compounding simultaneously:

> "We're seeing five to 10x every single year. Moore's law was two times every year and a half. In the case of AI, over the course of 10 years is probably 100,000 to a million x. And that's just the hardware. Then the next layer is the algorithm layer..."

According to [Epoch AI](https://epoch.ai/blog/algorithmic-progress-in-language-models), language model algorithmic efficiency is doubling every 8 months. The [Stanford AI Index](https://hai.stanford.edu/ai-index/2025-ai-index-report) reports inference costs dropped 280x in two years. DeepSeek trained a frontier model for $5.6M when GPT-4 cost an estimated $78M+.

Hardware improving. Algorithms improving. Architecture improving. All compounding.

## What Does That Look Like?

I made some crude projections. This isn't rigorous - it's back-of-envelope stuff to make the numbers visceral.

![Progress Scenarios](/assets/images/progress-scenarios.png)

The baseline is Moore's Law: roughly 10x per decade, the historical norm for computing. Current AI progress looks more like 100x per decade across the relevant metrics. The other lines show what happens if the rate of progress itself increases - not a given, but not obviously impossible either.

Jensen's "100,000 to a million x over 10 years" sits somewhere in the upper scenarios.

## Why This Matters

I don't really care about AGI timelines. But the steep slope is interesting regardless of where it leads.

The problem with most AI discourse is that people point to what they believe without being specific about what it would entail. "AI will transform everything" or "AI is overhyped" - neither tells you what to actually expect or how to act.

Plotting it out, even crudely, at least lets you think in terms of scenarios. If hardware and algorithms keep compounding at current rates, what does that mean for local inference in 3 years? 5 years? What decisions would you make differently?

The uncomfortable part is that humans are [provably bad](https://pmc.ncbi.nlm.nih.gov/articles/PMC8386158/) at intuiting exponentials. Studies during COVID found over 90% of people drastically underestimated infection spread - even educated participants who were explicitly warned about exponential bias. Our heuristics evolved in environments where compound growth was rare.

So when I try to predict forward, I assume my intuitions are wrong. The maths doesn't care what feels reasonable.