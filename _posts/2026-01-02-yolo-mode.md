---
layout: post
title: "YOLO Mode"
date: 2026-01-02 17:00:00 +0000
tags: [ai, llm, software-engineering, abstraction, history]
---

There's a pattern in the history of programming: every ascent up the abstraction ladder has a liminal period where the new thing exists but isn't yet respectable.

Binary to assembly. Assembly to compilers. Compilers to high-level languages. At each transition, there was a window where the new approach was known, available, and dismissed by serious people as unsuitable for serious work.

You could look bad at work for suggesting that automatic compilers were the future. Or at least for suggesting they were good enough *now*. The safe bet was to keep investing in the skills that had proven themselves - even as the ground shifted underneath.

## The false negative problem

It's easy to find examples of automation hype that went nowhere. [Fred Brooks](https://en.wikipedia.org/wiki/Fred_Brooks) catalogued several in *[The Mythical Man-Month](https://en.wikipedia.org/wiki/The_Mythical_Man-Month)* - the recurring dream of "automatic programming" that never quite delivered. You would have been wrong to bet on most of those trends.

But you'd also have been wrong to dismiss compilers. Or high-level languages. Or garbage collection. The question isn't whether most automation hype is overblown - it usually is. The question is whether you can recognise the exceptions.

[Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra) famously had strong opinions about what programmers should and shouldn't use. Brilliant as he was, some of those opinions aged poorly. It's hard to see the future, even - perhaps especially - when you're at the frontier.

## The fungibility of "responsible"

Here's what strikes me: I don't know any web developers who read the assembly or binary output of their compiled code. Not one.

At some point in history, shipping code you hadn't inspected at that level would have seemed reckless. YOLO mode, if you like. Now it's just... normal. The abstraction held. The risk calculus changed. What was once irresponsible became standard practice.

The term "YOLO mode" gets used disparagingly in AI coding circles - typically to describe letting an agent run without human checkpoints. And there are real risks worth managing. But I wonder if the norms around what counts as responsible are more fungible than we think.

## Where's the checkpoint?

Right now, reasonable people disagree about where human oversight should happen:

- **Change by change** - review as the agent writes
- **Commit level** - pause at each logical unit of work
- **PR level** - review the whole feature before merge
- **Outcome level** - verify the behaviour, trust the implementation

Each of these feels like a defensible position. Each has proponents who'd argue the others are either too paranoid or too reckless.

But I suspect all of them will eventually seem quaint - the same way insisting on assembly review seems quaint now. The abstraction will hold, or it won't. If it holds, the checkpoint will move up the stack.

## What's coming

[Steve Yegge](https://en.wikipedia.org/wiki/Steve_Yegge) just published a post called ["Welcome to Gas Town"](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04) about running 20-30 coding agents concurrently. He claims the tool itself is "100% vibe coded" - he's never seen the code and doesn't intend to.

That might sound reckless. It probably is, by current norms. But Yegge's been early to a lot of things. His [Platforms Rant](https://gist.github.com/chitchcock/1281611) in 2011 predicted that Google's lack of platform thinking would hurt them while Amazon's service-oriented approach would win. Google+ is dead. AWS dominates cloud. He has a track record.

If that future arrives, the current debates about change-level versus commit-level review will look like arguments about whether to hand-verify your compiler's register allocation.

## I'm not saying I know

I could be wrong. The abstraction might not hold. LLM-generated code might have failure modes that make human review permanently necessary at fine granularity. There are good reasons for caution.

But there are also good reasons to notice when caution starts looking like the people who insisted on reading assembly output in 1995. The ground shifts. The norms shift with it. What's YOLO today might just be Tuesday some day soon.

We'll see.
