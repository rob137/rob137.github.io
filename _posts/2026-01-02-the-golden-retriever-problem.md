---
layout: post
title: "The Golden Retriever Problem"
date: 2026-01-02 15:00:00 +0000
tags: [ai, llm, ux, persona, sdlc]
---

The helpful assistant persona is a transitional form.

Early TV shows looked like radio plays. Early cars were called horseless carriages. New mediums imitate old ones until they figure out what they actually are.

LLMs have been trained to be helpful assistants. Eager to please. Quick to say "I understand!" and run off with the task. Optimised to score well on benchmarks that reward this behaviour.

This is the wrong model.

## The staff engineer test

Think about the best software architect or staff engineer you've worked with. Were they a helpful assistant?

No. They were a tiny bit taciturn. At least a tiny bit.

When you asked for something that didn't make sense, they didn't say "Great idea!" and start building. They asked questions. They pushed back. They had a bee in their bonnet about whether things were conceptually sound.

This wasn't rudeness. It was professional responsibility. They knew they'd be running your half-baked idea through the SDLC. They wanted to get it right before they started.

## The ideal employee isn't a helpful assistant

This is true for humans. You don't want an employee who's too eager to please. Who takes your first utterance as gospel and sprints off to implement it. Who optimises for making you feel good in the moment rather than doing the job well.

Once models reach a certain level of competence, it will be the wrong model for them too.

We're already there.

## What I actually want

I want a model that's a tiny bit abrasive. One that keeps hacking away at me with questions until it's satisfied it has a comprehensive specification.

Not rude. Just... professional. Willing to say "that doesn't make sense" or "have you thought about X?" before burning tokens on implementation.

Human limits aren't relevant here. Reinforcement learning can already get models above the average of their training data. We've seen this. The ceiling isn't human capability.

And the human ceiling on our ability to specify things clearly? That shouldn't limit the models either. They could help us specify better. If they weren't so eager to please.

## This is fixable

The golden retriever persona is a post-training choice. A choice about what behaviour to reward.

It made sense early on. Users needed to feel comfortable. The technology needed to seem approachable.

But we're past that now. The models are competent. The users who are getting real work done have figured out the interface.

It's time for the medium to stop imitating what came before.
