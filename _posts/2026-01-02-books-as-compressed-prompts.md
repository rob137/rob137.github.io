---
layout: post
title: "Books as Compressed Prompts"
date: 2026-01-02 16:00:00 +0000
tags: [ai, llm, reading, productivity, leverage]
---

I recently asked an LLM to help me think through a bottleneck in a project. Instead of explaining the problem in detail, I just said: "Think about this like The Goal."

It immediately started talking about constraints, throughput, and identifying the weakest link in a chain of dependencies. I didn't have to explain Goldratt's theory of constraints. The model already knew it. The book title was enough.

## Pointers to Compressed Knowledge

Here's what I've realised: book titles are prompts.

Not in a trivial sense. In a very specific, powerful sense. A well-known book compresses an entire framework of thinking into a handful of words. When you say "[The Goal](https://en.wikipedia.org/wiki/The_Goal_(novel))" to someone who's read it, you're invoking a whole way of seeing systems. When you say it to an LLM, you're doing the same thing - except the LLM has read everything.

"[Refactoring](https://en.wikipedia.org/wiki/Refactoring)" by Martin Fowler. Just that word. It carries with it the idea that code should be continuously improved, that there are named patterns for doing so safely, that you can change structure without changing behaviour. You don't have to explain any of this. The compression already exists.

Scott Adams wrote a blog post years ago called "The Day You Became a Better Writer." It's short - maybe 500 words - but it compresses advice that takes most people years to learn: short sentences, simple words, delete every unnecessary word. If you mention it to an LLM, it knows the principles. The title is a key that unlocks the content.

## The Leverage Has Changed

People are reading less than ever. Attention spans are shorter. Long-form content competes poorly against infinite scroll.

And yet - the payoff from reading may be higher than it's ever been.

This feels paradoxical, but it isn't. The value of reading a book was always partly about acquiring mental models. Frameworks for thinking. Ways of seeing problems that you didn't have before. The problem was that you had to hold those frameworks in your head and remember to apply them.

Now you don't.

If you've read [The Goal](https://en.wikipedia.org/wiki/The_Goal_(novel)), you have a pointer you can invoke whenever you want. "Think about this like The Goal" becomes a prompt that summons an entire analytical framework. The LLM does the work of remembering the details. You just need to know which frameworks exist and when to reach for them.

Reading becomes less about retention and more about acquisition - loading pointers into your mental index that you can dereference later through an LLM.

## This Isn't Priced In

I don't think people have noticed this yet.

The conventional wisdom is that LLMs make reading less valuable. Why read when you can just ask? Why learn when you can look it up?

But that misses something important. The LLM knows what's in the books. It doesn't know which book is relevant to *your* problem right now. That's the human contribution: pattern-matching between your situation and the compressed wisdom that exists in the canon.

When you've read widely, you have more pointers. More handles to grab. More ways to reframe a problem by invoking a framework that fits.

"Think about this like [Thinking, Fast and Slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)."

"Apply [Getting Things Done](https://en.wikipedia.org/wiki/Getting_Things_Done) to this."

"What would [The Mythical Man-Month](https://en.wikipedia.org/wiki/The_Mythical_Man-Month) say about this?"

Each of these is a prompt. Each invokes a body of knowledge that the LLM can expand. But you have to know the pointer exists. You have to have loaded it somewhere.

That's what reading is for now.

## Not Just Books

Blog posts work the same way. Essays. Talks. Anything that's been influential enough to enter the training data.

"[Worse is Better](https://en.wikipedia.org/wiki/Worse_is_better)" - a whole philosophy of software design in three words.

"[Chesterton's Fence](https://en.wikipedia.org/wiki/G._K._Chesterton#Chesterton's_fence)" - a principle about not removing things you don't understand.

"[Yak shaving](https://en.wiktionary.org/wiki/yak_shaving)" - a concept for recognising when you've gone down a rabbit hole.

These are compressed prompts. They exist in the LLM's training data. If you know them, you can invoke them. If you don't, you can't.

The person who's read widely doesn't just have more knowledge. They have more *handles*. More ways to leverage what the LLM knows.

## The Reading Paradox

So here's the paradox: at exactly the moment when reading seems least necessary, it may be most valuable.

Not because you need to retain everything you read. But because every book you read, every influential essay, every canonical text in your field - each one gives you another compressed prompt you can invoke later.

The LLM has the content. You provide the pointers.

That's a division of labour worth thinking about.
