---
layout: post
title: "The Expanding Universe"
date: 2026-01-04 14:00:00 +0000
tags: [ai, coding, domains, spectrum]
excerpt: "Some builders have flipped. Others haven't. The pattern tells you something about where they work - and when they last looked."
---

I wrote a few days ago about [crossing a threshold](/2026/01/02/crossing-the-horizon/) - something changing in late 2025 that wasn't fully recognised yet. Since then I've been watching who's flipped and who hasn't.

## The flippers

[Andrej Karpathy](https://en.wikipedia.org/wiki/Andrej_Karpathy), October 2025, on his nanochat project. He described it as "intellectually intense code" where "everything has to be very precisely arranged." AI was "net unhelpful." He called the output "slop."

[Andrej Karpathy](https://x.com/karpathy/status/2004607146781278521), December 2025:

> "I've never felt this much behind as a programmer. The profession is being dramatically refactored."

Two months apart.

[DHH](https://en.wikipedia.org/wiki/David_Heinemeier_Hansson), summer 2025:

> "Not letting AI write any code directly."

[DHH](https://x.com/dhh), January 2026:

> "Half the resistance was simply that the models weren't good enough yet... That has now flipped. Working with opencode has been a blast. Watching the thinking models nail a difficult bug is a revelation."

## The holdouts

[George Hotz](https://en.wikipedia.org/wiki/George_Hotz), on the Lex Fridman podcast:

> "GPT is great for quick generic scripts, but not for complex tasks."

He's noted that AI may help with "typing-speed-limited" work, but that's not where his bottleneck is. His work on [tinygrad](https://github.com/tinygrad/tinygrad) is low-level, performance-critical, novel.

[ThePrimeagen](https://en.wikipedia.org/wiki/ThePrimeagen), also on Lex Fridman:

> AI "often gets it wrong, especially with larger, more complex codes... lacks deep understanding for problem-solving and design."

He sees some utility. Not transformative.

[John Blow](https://en.wikipedia.org/wiki/Jonathan_Blow), [consistently](/2026/01/03/the-witnesses/):

> "Current AIs can't code. It's clear by now that everyone who thinks they can are not good programmers themselves, and/or only ever do trivial problems."

## The pattern

The flippers are in web, enterprise, applications. The holdouts are in systems programming, game engines, low-level optimisation.

Both can be right. The experiences are genuinely different depending on what you build.

But there's another variable: when did you last look?

Karpathy explicitly crossed a threshold between October and December. DHH says the models "weren't good enough yet" and "that has now flipped." They circled back.

I don't know if Hotz, Primeagen, or Blow have tried the latest thinking models on their hardest problems. Maybe they have and the answer is still no. Maybe they haven't. The universe is expanding either way - practices are diverging, and so is the conversation.

## The question

If you're in a domain where AI hasn't clicked yet, it's worth asking: is that because of the domain, or because of when you last tried?

The threshold is real. Some people crossed it recently. Others might not have reached it yet.
