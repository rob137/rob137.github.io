---
layout: post
title: "Show, Don't Tell"
date: 2026-01-02 16:30:00 +0000
tags: [ai, llm, decision-making, musings]
---

Some incomplete musings. I haven't fully processed this yet, but it feels worth sharing.

## The optimizer trap

I'm a particular kind of frugal. I'm happy to spend money where there's a payoff - I'd rather frame cost efficiency on a decades level than skimp on purchases. A Makita circular saw instead of something cheap that stops working. Prosumer territory.

[Venkatesh Rao](https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/) called this "premium mediocre" - products respected but not favoured by professionals, because professionals are paid to worry about the final 5% that I don't care about. My Withings ScanWatch. My Kamei roof box. The product that if you know, you know.

The problem is that optimizing is a trap. There's an analogy of ants following a pheromone trail in a circle until they dehydrate and die. I've noticed a softer version of this in myself - hunting for the 5% better product, chasing failure modes and trade-offs, but moving in a circle because no product is actually perfect.

I'm mindful of this now. For unimportant categories, I deliberately satisfice with the first good-enough product I find. For important ones - professional tools, things that affect quality of life - I let myself go deep.

## The two-hour loop

Recently I needed something in the "important" category. I spent about two hours talking to an LLM, chasing the question of what to buy.

The process: present the problem, get suggestions, chase the question about each suggestion until I find a caveat, pose the caveat back, repeat. I became convinced three separate products were correct before finding the failure mode in each and moving on. The fourth stuck.

Good decision. Bought it. All well and good.

Then I had a moment of disquiet.

## What if I'd just shown it?

The model knew nothing about me other than my immediate problem. What if I'd told it what I told you just now - how I approach decisions, the bite points between cost and value I'm looking for?

More than that: what if I'd just listed products I've bought in the past - the watch, the roof box, the power tools - without explaining why?

So I tried it. New chat, no history. Shared the need. Listed some past purchases without commentary. Asked for one recommendation.

It immediately suggested the product I'd reached after two hours of iteration.

## Drawing parallels

Something interesting is happening here. The model drew out a pattern from examples I gave without explaining the pattern. It inferred my decision-making style from the products themselves.

Humans can do this. It feels like a very human thing. But most people I know would struggle with it - partly because being up-to-date on products matters, partly because it requires depth across domains.

The models have that depth now.

## Thinking too small

The obvious implication is marketing. You could anticipate a solid purchasing decision for me and present it before I get lost in a two-hour loop. I'd be happy if you did.

But I suspect this is thinking too small.

What if a model proactively chased the question of what questions aren't being asked? What if it suggested to an aging martial artist with joint issues that they consider a cycling club - even though it seems like a category error - because the concerns they've raised are actually addressed well by that unexplored territory?

This connects to [the golden retriever problem](/2026/01/02/the-golden-retriever-problem/). You don't necessarily want a model that avoids annoying you. You might want something a little obstructive - like how friends shake you by the shoulders when you're missing the point.

The diva with the entourage. The dictator surrounded by yes-men. I wonder if there's a shade of this in what's been called "LLM psychosis" - the way people can drift into strange places when they only talk to systems designed to accommodate them.

## Open questions

Maybe one antidote is models that are less accommodating. But maybe another is models that are better at associative thinking - that can raise the idea that what seems like a category error might actually be relevant.

It feels like there are already hints of this in current models. I don't know if it's emergent or designed. I leave it open.

These are musings, not conclusions. But the pace at which things are moving makes me want to share them anyway.
